{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nmmo\n",
    "from nmmo import config\n",
    "\n",
    "from implementations.train_ppo import train_ppo, evaluate_agent\n",
    "from implementations.SimplierInputAgentV2 import SimplierInputAgentV2\n",
    "from implementations.RandomAgent import get_avg_lifetime_for_random_agent, get_avg_reward_for_random_agent\n",
    "from implementations.Observations import Observations\n",
    "from implementations.CustomRewardBase import LifetimeReward, ResourcesAndGatheringReward, \\\n",
    "    ExplorationReward, WeightedReward, ShiftingReward, CurriculumReward, ResourcesReward, \\\n",
    "    CustomRewardBase\n",
    "from implementations.StayNearResourcesReward import StayNearResourcesReward\n",
    "from implementations.SavingCallback import SavingCallback\n",
    "from implementations.AnimationCallback import AnimationCallback\n",
    "from implementations.PathTrackingCallback import PathTrackingCallback\n",
    "from implementations.SpawnTrackingCallback import SpawnTrackingCallback\n",
    "from implementations.observations_to_inputs import observations_to_inputs_simplier\n",
    "from implementations.plots import *\n",
    "from implementations.jar import Jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config.Default()\n",
    "conf.set(\"PLAYER_N\", 32)\n",
    "#conf.set(\"NPC_N\", 0)\n",
    "\n",
    "# lifetime_reward = LifetimeReward(max_lifetime=1000)\n",
    "# resources_reward = ResourcesReward(max_lifetime=1000)\n",
    "# exploration_reward = ExplorationReward(max_lifetime=1000, map_size=128, view_radius=7)\n",
    "\n",
    "# random_lifetime_reward, _ = get_avg_reward_for_random_agent(conf, reward=lifetime_reward, retries=20)\n",
    "# print(f\"Random agent lifetime reward: {random_lifetime_reward:.6f}\")\n",
    "# random_resources_reward, _ = get_avg_reward_for_random_agent(conf, reward=resources_reward, retries=20)\n",
    "# print(f\"Random agent resources reward: {random_resources_reward:.6f}\")\n",
    "# random_exploration_reward, _ = get_avg_reward_for_random_agent(conf, reward=exploration_reward, retries=20)\n",
    "# print(f\"Random agent exploration reward: {random_exploration_reward:.6f}\")\n",
    "\n",
    "# reward = CurriculumReward(reward_stages=[\n",
    "#     (3.5 * random_resources_reward, resources_reward),\n",
    "#     (3 * random_exploration_reward, exploration_reward),\n",
    "#     (0, lifetime_reward)\n",
    "# ])\n",
    "\n",
    "reward = WeightedReward({\n",
    "    StayNearResourcesReward(1024, target_distance=2): 1,\n",
    "    ResourcesAndGatheringReward(1024, gathering_bonus=4, scale_with_resource_change=True): 1,\n",
    "    ExplorationReward(1024): 0.2\n",
    "})\n",
    "\n",
    "random_reward, random_rewards = get_avg_reward_for_random_agent(conf, reward=reward, retries=20)\n",
    "random_reward_std = np.std(random_rewards)\n",
    "print(f\"Random agent reward: {random_reward:.6f} ± {random_reward_std:.6f}\")\n",
    "\n",
    "random_lifetime, random_lifetimes = get_avg_lifetime_for_random_agent(conf, retries=20)\n",
    "random_lifetime_std = np.std(random_lifetimes)\n",
    "print(f\"Random agent lifetime: {random_lifetime:4.2f} ± {random_lifetime_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent() -> SimplierInputAgentV2:\n",
    "    return SimplierInputAgentV2(\n",
    "                learning_rate=5e-5,\n",
    "                lr_decay=0.999,\n",
    "                min_lr=5e-7,\n",
    "                critic_learning_rate=5e-6,\n",
    "                critic_lr_decay=0.999,\n",
    "                critic_min_lr=5e-7,\n",
    "                epsilon=0.1,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                entropy_loss_coef=3e-5,\n",
    "                max_grad_norm=0.5,\n",
    "                sample_weights_softmin_temp=-0.5,\n",
    "                action_loss_weights={\n",
    "                    \"Move\": 2.5,\n",
    "                    \"AttackStyle\": 0.5,\n",
    "                    \"AttackTargetPos\": 0.5,\n",
    "                })\n",
    "    \n",
    "def make_test_configs() -> dict[str, SimplierInputAgentV2]:\n",
    "    configs = {}\n",
    "    \n",
    "    # no lr decay\n",
    "    no_lr_decay = make_agent()\n",
    "    no_lr_decay.lr_decay = 1\n",
    "    no_lr_decay.critic_lr_decay = 1\n",
    "    configs[\"no_lr_decay\"] = no_lr_decay\n",
    "    \n",
    "    # no entropy loss\n",
    "    no_entropy_loss = make_agent()\n",
    "    no_entropy_loss.entropy_loss_coef = 0\n",
    "    configs[\"no_entropy_loss\"] = no_entropy_loss\n",
    "    \n",
    "    # no action loss weights\n",
    "    no_action_loss_weights = make_agent()\n",
    "    no_action_loss_weights.action_loss_weights = {}\n",
    "    configs[\"no_action_loss_weights\"] = no_action_loss_weights\n",
    "    \n",
    "    # no sample weights\n",
    "    no_sample_weights = make_agent()\n",
    "    no_sample_weights.softmin_temp = 0\n",
    "    configs[\"no_sample_weights\"] = no_sample_weights\n",
    "    \n",
    "    # positive sample weights\n",
    "    positive_sample_weights = make_agent()\n",
    "    positive_sample_weights.softmin_temp = 1\n",
    "    configs[\"positive_sample_weights\"] = positive_sample_weights\n",
    "    \n",
    "    # more negative sample weights\n",
    "    more_negative_sample_weights = make_agent()\n",
    "    more_negative_sample_weights.softmin_temp = -1\n",
    "    configs[\"more_negative_sample_weights\"] = more_negative_sample_weights\n",
    "    \n",
    "    # no gradient clipping\n",
    "    no_gradient_clipping = make_agent()\n",
    "    no_gradient_clipping.max_grad_norm = 1000\n",
    "    configs[\"no_gradient_clipping\"] = no_gradient_clipping\n",
    "    \n",
    "    # no advantage normalization\n",
    "    no_advantage_normalization = make_agent()\n",
    "    no_advantage_normalization.normalize_advantages = False\n",
    "    configs[\"no_advantage_normalization\"] = no_advantage_normalization\n",
    "    \n",
    "    # no separate critic lr\n",
    "    no_separate_critic_lr = make_agent()\n",
    "    no_separate_critic_lr.critic_learning_rate = no_separate_critic_lr.learning_rate\n",
    "    no_separate_critic_lr.critic_lr_decay = no_separate_critic_lr.lr_decay\n",
    "    no_separate_critic_lr.critic_min_lr = no_separate_critic_lr.min_lr\n",
    "    configs[\"no_separate_critic_lr\"] = no_separate_critic_lr\n",
    "    \n",
    "def run_test_configs(\n",
    "    reward: CustomRewardBase, \n",
    "    run_only: list[str] | None = None,\n",
    "    episodes: int = 400,\n",
    "    dir_name: str = \"final_tests\"\n",
    ") -> None:\n",
    "    configs = make_test_configs()\n",
    "    if run_only is not None:\n",
    "        configs = {k: v for k, v in configs.items() if k in run_only}\n",
    "        \n",
    "    random_reward, random_rewards = get_avg_reward_for_random_agent(env_conf, reward=reward, retries=20)\n",
    "    random_reward_std = np.std(random_rewards)\n",
    "    print(f\"Random agent reward: {random_reward:.6f} ± {random_reward_std:.6f}\")\n",
    "\n",
    "    random_lifetime, random_lifetimes = get_avg_lifetime_for_random_agent(env_conf, retries=20)\n",
    "    random_lifetime_std = np.std(random_lifetimes)\n",
    "    print(f\"Random agent lifetime: {random_lifetime:4.2f} ± {random_lifetime_std:.2f}\")\n",
    "        \n",
    "    for name, agent in configs.items():\n",
    "        env_conf = config.Default()\n",
    "        env_conf.set(\"PLAYER_N\", 32)\n",
    "        \n",
    "        agent_name = f\"{dir_name}/{name}\"\n",
    "        train_ppo(nmmo.Env(env_conf),\n",
    "            agent,\n",
    "            episodes=episodes,\n",
    "            save_every=25,\n",
    "            print_every=5,\n",
    "            eval_every=50,\n",
    "            eval_episodes=5,\n",
    "            custom_reward=reward,\n",
    "            agent_name=agent_name,\n",
    "            callbacks=[\n",
    "                SavingCallback(agent_name, reward_config=reward.get_config())])\n",
    "        \n",
    "        \n",
    "        plot_rewards_from_save(agent_name, random_agent_reward=random_reward, window=50, save_as=f\"{dir_name}/rewards/{name}\")\n",
    "        plot_lifetimes_from_save(agent_name, random_agent_lifetime=random_lifetime, window=50, save_as=f\"{dir_name}/lifetimes/{name}\")\n",
    "        plot_losses_from_save(agent_name, window=1000, save_as=f\"{dir_name}/losses/{name}\")\n",
    "        plot_entropies_from_save(agent_name, window=1000, save_as=f\"{dir_name}/entropies/{name}\")\n",
    "        \n",
    "def make_plots_from_save(save_name: str) -> None:\n",
    "    dir_name = \"variants\"\n",
    "    plot_rewards_from_save(save_name, random_agent_reward=random_reward, window=50, save_as=f\"{dir_name}/rewards/{save_name}\")\n",
    "    plot_lifetimes_from_save(save_name, random_agent_lifetime=random_lifetime, window=50, save_as=f\"{dir_name}/lifetimes/{save_name}\")\n",
    "    plot_losses_from_save(save_name, window=1000, save_as=f\"{dir_name}/losses/{save_name}\")\n",
    "    plot_entropies_from_save(save_name, window=50, save_as=f\"{dir_name}/entropies/{save_name}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"variation_scaled_rathering_bonus\"\n",
    "save_name = agent_name\n",
    "\n",
    "train_ppo(nmmo.Env(conf),\n",
    "          SimplierInputAgentV2(\n",
    "            learning_rate=5e-5,\n",
    "            lr_decay=0.999,\n",
    "            min_lr=5e-7,\n",
    "            critic_learning_rate=5e-6,\n",
    "            critic_lr_decay=0.999,\n",
    "            critic_min_lr=5e-7,\n",
    "            epsilon=0.1,\n",
    "            epochs=50,\n",
    "            batch_size=256,\n",
    "            entropy_loss_coef=3e-5,\n",
    "            max_grad_norm=0.5,\n",
    "            sample_weights_softmin_temp=-0.5,\n",
    "            action_loss_weights={\n",
    "                \"Move\": 2.5,\n",
    "                \"AttackStyle\": 0.5,\n",
    "                \"AttackTargetPos\": 0.5,\n",
    "              }),\n",
    "          episodes=2000,\n",
    "          save_every=25,\n",
    "          print_every=5,\n",
    "          eval_every=50,\n",
    "          eval_episodes=5,\n",
    "          custom_reward=reward,\n",
    "          agent_name=agent_name,\n",
    "          callbacks=[\n",
    "            SavingCallback(save_name, reward_config=reward.get_config())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_from_save(save_name, random_agent_reward=random_reward, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lifetimes_from_save(save_name, random_agent_lifetime=random_lifetime, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_from_save(save_name, window=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entropies_from_save(save_name, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spawn_callback = SpawnTrackingCallback()\n",
    "path_callback = PathTrackingCallback()\n",
    "\n",
    "eval_rewards, _ = evaluate_agent(\n",
    "    nmmo.Env(conf),\n",
    "    SimplierInputAgentV2.load(f\"{agent_name}_best\"),\n",
    "    episodes=20,\n",
    "    custom_reward=LifetimeReward(1024),\n",
    "    callbacks=[\n",
    "        spawn_callback,\n",
    "        path_callback,\n",
    "        AnimationCallback(1, f\"{agent_name}_best_animation\")\n",
    "    ],\n",
    "    sample_actions=True\n",
    ")\n",
    "\n",
    "print(f\"Average reward: {np.mean(eval_rewards):.6f} ± {np.std(eval_rewards):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spawn_callback.plot_density_reward_correlation(max_distance=50, smoothing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_callback.plot_paths(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_observations_from_save(save_name: str, agent_ids: list[int]) -> list[Observations]:       \n",
    "    history = Jar(\"saves\").get(save_name)\n",
    "    observations = [ep_obs[agent_id] \n",
    "                   for ep in history \n",
    "                   for ep_obs, _ in ep[0] \n",
    "                   for agent_id in agent_ids\n",
    "                   if agent_id in ep_obs]\n",
    "    return observations\n",
    "\n",
    "def verify_observations(save_name: str) -> None:\n",
    "    observations = get_all_observations_from_save(save_name, agent_ids=list(range(1, 33)))\n",
    "    net_inputs = [observations_to_inputs_simplier(obs, device=\"cpu\") for obs in observations]\n",
    "\n",
    "    tiles = [inp[0][0] for inp in net_inputs]\n",
    "    tile_features = [feature for tile in tiles for feature in tile.reshape(-1, 28)[:, -9:]if not torch.all(feature == 0)]\n",
    "\n",
    "    self_datas = [inp[1][0] for inp in net_inputs]\n",
    "    move_masks = [inp[2][0] for inp in net_inputs]\n",
    "    attack_masks = [inp[3][0] for inp in net_inputs]\n",
    "\n",
    "    def assert_for_all(values, assertion_fn, description):\n",
    "        correct_count = sum([assertion_fn(tensor) for tensor in values])\n",
    "        total_count = len(values)\n",
    "        print(f\"{(description+':'):<35}{correct_count}/{total_count} {('✅' if correct_count == total_count else '❌')}\")\n",
    "\n",
    "    assert_for_all(tiles, lambda x: x.shape == torch.Size([15, 15, 28]), \"Tiles shape\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, :16], dim=-1) == 1), \"16 features one-hot encoded\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, 16:18], dim=-1) == 1), \"Each tile either passable or not\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.logical_or(x[:, :, 18] == 0, x[:, :, 18] == 1)), \"Each tile harvestable or not\")\n",
    "    # TODO: Check seen entity data\n",
    "    print()\n",
    "\n",
    "    assert_for_all(self_datas, lambda x: x.shape == torch.Size([5]), \"Self data shape\")\n",
    "    assert_for_all(self_datas, lambda x: torch.all((x >= 0) & (x <= 1)), \"All values between 0 and 1\")\n",
    "    print()\n",
    "\n",
    "    assert_for_all(attack_masks, lambda x: x.shape == torch.Size([3]), \"Attack mask shape\")\n",
    "    assert_for_all(attack_masks, lambda x: torch.all(x == 1), \"Every attack style valid\")\n",
    "    print()\n",
    "\n",
    "    assert_for_all(move_masks, lambda x: x.shape == torch.Size([5]), \"Move mask shape\")\n",
    "    assert_for_all(move_masks, lambda x: x[-1] == 1, \"Can not move\")\n",
    "    assert_for_all(move_masks, lambda x: torch.any(x[:-1] == 1), \"Can move somewhere\")\n",
    "\n",
    "    seen_ids = {}\n",
    "    for obs in observations:\n",
    "        if obs.agent_id not in seen_ids:\n",
    "            seen_ids[obs.agent_id] = {}\n",
    "            \n",
    "        for seen_id in obs.entities.id:\n",
    "            if seen_id == 0 or seen_id == obs.agent_id:\n",
    "                continue\n",
    "            \n",
    "            if seen_id not in seen_ids[obs.agent_id]:\n",
    "                seen_ids[obs.agent_id][seen_id] = 0\n",
    "                \n",
    "            seen_ids[obs.agent_id][seen_id] += 1\n",
    "            \n",
    "    # Check that if one agent sees another, the other agent sees the first agent\n",
    "    for agent_id, seen in seen_ids.items():\n",
    "        for seen_id, count in seen.items():\n",
    "            if seen_ids.get(seen_id, {}).get(agent_id, 0) != count:\n",
    "                print(f\"Agent {agent_id} saw {seen_id} {count} times, but {seen_id} saw {agent_id} {seen_ids.get(seen_id, {}).get(agent_id, 0)} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
