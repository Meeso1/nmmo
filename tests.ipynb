{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nmmo\n",
    "from nmmo import config\n",
    "\n",
    "from implementations.train_ppo import train_ppo, evaluate_agent\n",
    "from implementations.SimplierInputAgentV2 import SimplierInputAgentV2\n",
    "from implementations.RandomAgent import get_avg_lifetime_for_random_agent, get_avg_reward_for_random_agent\n",
    "from implementations.Observations import Observations\n",
    "from implementations.CustomRewardBase import LifetimeReward, \\\n",
    "    ResourcesAndGatheringReward, ExplorationReward, WeightedReward, ShiftingReward, CurriculumReward, ResourcesReward\n",
    "from implementations.StayNearResourcesReward import StayNearResourcesReward\n",
    "from implementations.SavingCallback import SavingCallback\n",
    "from implementations.AnimationCallback import AnimationCallback\n",
    "from implementations.PathTrackingCallback import PathTrackingCallback\n",
    "from implementations.observations_to_inputs import observations_to_inputs_simplier\n",
    "from implementations.jar import Jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_observations_from_save(save_name: str, agent_ids: list[int]) -> list[Observations]:       \n",
    "    history = Jar(\"saves\").get(save_name)\n",
    "    observations = [ep_obs[agent_id] \n",
    "                   for ep in history \n",
    "                   for ep_obs, _ in ep[0] \n",
    "                   for agent_id in agent_ids\n",
    "                   if agent_id in ep_obs]\n",
    "    return observations\n",
    "\n",
    "def get_entropies_from_save(save_name: str) -> dict[str, list[float]]:     \n",
    "\thistory = Jar(\"saves\").get(save_name)\n",
    "\tentropies_per_episode = [[entropies \n",
    "                           \t\tfor step in ep[4] \n",
    "                           \t\tfor entropies in step.values()]\n",
    "\t\t\t\t\t\t    for ep in history]\n",
    " \n",
    "\tmeans_per_episode_per_type = [{type: np.mean([entropies[type] for entropies in ep]) \n",
    "                                  \tfor type in ep[0].keys()} \n",
    "                                 for ep in entropies_per_episode]\n",
    " \n",
    "\tmeans_per_type_per_episode = {type: [ep[type] for ep in means_per_episode_per_type]\n",
    "                                  for type in means_per_episode_per_type[0].keys()}\n",
    "\treturn means_per_type_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(\n",
    "    actor_losses: list[float], \n",
    "    critic_losses: list[float],\n",
    "    window: int = 500\n",
    ") -> None:\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    ax1.plot(actor_losses, label=\"Actor Loss\", color='blue', alpha=0.4)\n",
    "    actor_losses_smooth = np.convolve(actor_losses, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    actor_losses_std = np.array([np.std(actor_losses[max(0, i-window):i+1]) \n",
    "                                for i in range(window-1, len(actor_losses))])\n",
    "    \n",
    "    ax1.plot(range(window-1, len(actor_losses)), actor_losses_smooth, \n",
    "             label=f\"Running Mean (window={window})\", color='red')\n",
    "    ax1.fill_between(range(window-1, len(actor_losses)), \n",
    "                     actor_losses_smooth - actor_losses_std,\n",
    "                     actor_losses_smooth + actor_losses_std,\n",
    "                     alpha=0.2, color='red', label='Standard Deviation')\n",
    "    \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Actor Loss Over Time\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.semilogy(critic_losses, label=\"Critic Loss\", color='blue', alpha=0.4)\n",
    "    critic_losses_smooth = np.convolve(critic_losses, np.ones(window)/window, mode='valid')\n",
    "    ax2.semilogy(range(window-1, len(critic_losses)), critic_losses_smooth, \n",
    "                 label=f\"Running Mean (window={window})\", color='red')\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Loss (log scale)\")\n",
    "    ax2.set_title(\"Critic Loss Over Time\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_losses_from_save(agent_name: str, window: int = 500) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    losses = [episode[2] for episode in history]\n",
    "    actor_losses = [l for loss in losses for l in loss[0]]\n",
    "    critic_losses = [l for loss in losses for l in loss[1]]\n",
    "    \n",
    "    plot_losses(actor_losses, critic_losses, window)\n",
    "\n",
    "def plot_rewards(\n",
    "    avg_rewards: list[float], \n",
    "    max_rewards: list[float], \n",
    "    min_rewards: list[float], \n",
    "    ninetieth_percentile_rewards: list[float] | None = None, \n",
    "    random_agent_reward: float | None = None,\n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if ninetieth_percentile_rewards is not None:\n",
    "        ax.plot(ninetieth_percentile_rewards, label=\"90th Percentile Reward\", color='purple', alpha=0.4)\n",
    "        \n",
    "    ax.plot(avg_rewards, label=\"Average Reward\", color='red', alpha=0.4)\n",
    "    ax.plot(max_rewards, label=\"Max Reward\", color='pink', alpha=0.8)\n",
    "    ax.plot(min_rewards, label=\"Min Reward\", color='green', alpha=0.4)\n",
    "    \n",
    "    if random_agent_reward is not None:\n",
    "        ax.axhline(y=random_agent_reward, label=\"Random Agent Reward\", color='black', linestyle='--')\n",
    "    \n",
    "    avg_rewards_smooth = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(avg_rewards)), avg_rewards_smooth, label=f\"Running Mean (window={window})\", color='blue')\n",
    "\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    ax.set_title(\"Rewards Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_rewards_from_save(agent_name: str, window: int = 50, random_agent_reward: float | None = None) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    rewards = [episode[1] for episode in history]\n",
    "    num_agents = len(rewards[0])\n",
    "    \n",
    "    avg_rewards = [np.mean([r for r in reward.values()]) for reward in rewards]\n",
    "    max_rewards = [np.max([r for r in reward.values()]) for reward in rewards]\n",
    "    min_rewards = [np.min([r for r in reward.values()]) for reward in rewards]\n",
    "    \n",
    "    if num_agents > 30:\n",
    "        ninetieth_percentile_rewards = [np.percentile([r for r in reward.values()], 90) for reward in rewards]\n",
    "    else:\n",
    "        ninetieth_percentile_rewards = None\n",
    "    \n",
    "    plot_rewards(avg_rewards, max_rewards, min_rewards, ninetieth_percentile_rewards, random_agent_reward, window)\n",
    "\n",
    "def plot_lifetimes(\n",
    "    avg_lifetimes: list[float], \n",
    "    max_lifetimes: list[float], \n",
    "    min_lifetimes: list[float], \n",
    "    ninetieth_percentile: list[float] | None = None,\n",
    "    random_agent_lifetime: float | None = None,\n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if ninetieth_percentile is not None:\n",
    "        ax.plot(ninetieth_percentile, label=\"90th Percentile\", color='purple', alpha=0.4)\n",
    "        \n",
    "    ax.plot(avg_lifetimes, label=\"Average Lifetime\", color='red', alpha=0.4)\n",
    "    ax.plot(max_lifetimes, label=\"Max Lifetime\", color='pink', alpha=0.8)\n",
    "    ax.plot(min_lifetimes, label=\"Min Lifetime\", color='green', alpha=0.4)\n",
    "    \n",
    "    if random_agent_lifetime is not None:\n",
    "        ax.axhline(y=random_agent_lifetime, label=\"Random Agent Lifetime\", color='black', linestyle='--')\n",
    "\n",
    "    avg_rewards_smooth = np.convolve(avg_lifetimes, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(avg_lifetimes)), avg_rewards_smooth, label=f\"Running Mean (window={window})\", color='blue')\n",
    "\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Lifetime\")\n",
    "    ax.set_title(\"Agent Lifetime Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()   \n",
    "\n",
    "def plot_lifetimes_from_save(agent_name: str, random_agent_lifetime: float | None = None, window: int = 50) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    lifetimes = [episode[3] for episode in history]\n",
    "    num_agents = len(lifetimes[0])\n",
    "    \n",
    "    avg_lifetimes = [np.mean([r for r in reward.values()]) for reward in lifetimes]\n",
    "    max_lifetimes = [np.max([r for r in reward.values()]) for reward in lifetimes]\n",
    "    min_lifetimes = [np.min([r for r in reward.values()]) for reward in lifetimes]\n",
    "    \n",
    "    if num_agents > 30:\n",
    "        ninetieth_percentile = [np.percentile([r for r in lifetime.values()], 90) for lifetime in lifetimes]\n",
    "    else:\n",
    "        ninetieth_percentile = None\n",
    "    \n",
    "    plot_lifetimes(avg_lifetimes, max_lifetimes, min_lifetimes, ninetieth_percentile, random_agent_lifetime, window)\n",
    "    \n",
    "def plot_entropies(\n",
    "    means_per_type_per_episode: dict[str, list[float]], \n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, axs = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for idx, (type, entropies) in enumerate(means_per_type_per_episode.items()):\n",
    "        ax = axs[idx]\n",
    "        ax.plot(entropies, label=f\"{type} Entropy\", alpha=0.4)\n",
    "        \n",
    "        entropies_smooth = np.convolve(entropies, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(entropies)), entropies_smooth, \n",
    "               label=f\"Running Mean (window={window})\")\n",
    "        \n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.set_ylabel(\"Entropy\")\n",
    "        ax.set_title(f\"{type} Entropy Over Time\")\n",
    "        ax.set_ylim(bottom=0)  # Set y-axis minimum to 0\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_entropies_from_save(agent_name: str, window: int = 50) -> None:\n",
    "    plot_entropies(get_entropies_from_save(agent_name), window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random agent lifetime reward: 0.089275\n",
      "Random agent resources reward: 0.070196\n",
      "Random agent exploration reward: 0.014573\n",
      "{'name': 'CurriculumReward', 'stages': [{'reward_threshold': 0.8, 'config': {'name': 'ResourcesReward', 'max_lifetime': 1000}}, {'reward_threshold': 0.13, 'config': {'name': 'ExplorationReward', 'max_lifetime': 1000, 'map_size': 128, 'view_radius': 7}}, {'reward_threshold': 0, 'config': {'name': 'LifetimeReward', 'max_lifetime': 1000}}]}\n",
      "Random agent reward: 0.065412 ± 0.012828\n",
      "Random agent lifetime: 85.41 ± 13.20\n"
     ]
    }
   ],
   "source": [
    "conf = config.Default()\n",
    "conf.set(\"PLAYER_N\", 32)\n",
    "#conf.set(\"NPC_N\", 0)\n",
    "\n",
    "# reward = WeightedReward({\n",
    "#     StayNearResourcesReward(1024, target_distance=1): 1,\n",
    "#     ResourcesAndGatheringReward(1024, gathering_bonus=4, scale_with_resource_change=True): 1,\n",
    "#     ExplorationReward(1024): 0.2\n",
    "# })\n",
    "\n",
    "lifetime_reward = LifetimeReward(max_lifetime=1000)\n",
    "resources_reward = ResourcesReward(max_lifetime=1000)\n",
    "exploration_reward = ExplorationReward(max_lifetime=1000, map_size=128, view_radius=7)\n",
    "\n",
    "random_lifetime_reward, _ = get_avg_reward_for_random_agent(conf, reward=lifetime_reward, retries=20)\n",
    "print(f\"Random agent lifetime reward: {random_lifetime_reward:.6f}\")\n",
    "random_resources_reward, _ = get_avg_reward_for_random_agent(conf, reward=resources_reward, retries=20)\n",
    "print(f\"Random agent resources reward: {random_resources_reward:.6f}\")\n",
    "random_exploration_reward, _ = get_avg_reward_for_random_agent(conf, reward=exploration_reward, retries=20)\n",
    "print(f\"Random agent exploration reward: {random_exploration_reward:.6f}\")\n",
    "\n",
    "# reward_stages = [\n",
    "#     (5 * random_resources_reward, resources_reward),\n",
    "#     (5 * random_exploration_reward, exploration_reward),\n",
    "#     (0, lifetime_reward)\n",
    "# ]\n",
    "# random_baselines = {\n",
    "#     \"LifetimeReward\": random_lifetime_reward,\n",
    "#     \"ResourcesReward\": random_resources_reward,\n",
    "#     \"ExplorationReward\": random_exploration_reward\n",
    "# }\n",
    "\n",
    "# curriculum_reward = CurriculumReward(\n",
    "#     reward_stages=[\n",
    "#         (1.2, resources_reward),\n",
    "#         (1.3, exploration_reward),\n",
    "#         (1.4, lifetime_reward)\n",
    "#     ],\n",
    "#     random_agent_baselines=random_baselines\n",
    "# )\n",
    "\n",
    "reward_stages = [\n",
    "    (0.8, resources_reward),\n",
    "    (0.13, exploration_reward),\n",
    "    (0, lifetime_reward)\n",
    "]\n",
    "\n",
    "curriculum_reward = CurriculumReward(reward_stages=reward_stages)\n",
    "\n",
    "print(curriculum_reward.get_config())\n",
    "reward = curriculum_reward\n",
    "\n",
    "\n",
    "random_reward, random_rewards = get_avg_reward_for_random_agent(conf, reward=reward, retries=20)\n",
    "random_reward_std = np.std(random_rewards)\n",
    "print(f\"Random agent reward: {random_reward:.6f} ± {random_reward_std:.6f}\")\n",
    "\n",
    "random_lifetime, random_lifetimes = get_avg_lifetime_for_random_agent(conf, retries=20)\n",
    "random_lifetime_std = np.std(random_lifetimes)\n",
    "print(f\"Random agent lifetime: {random_lifetime:4.2f} ± {random_lifetime_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Save with name 'curriculum_reward_30' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m agent_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurriculum_reward_30\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m save_name \u001b[38;5;241m=\u001b[39m agent_name\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnmmo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mSimplierInputAgentV2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcritic_learning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcritic_lr_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcritic_min_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mentropy_loss_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_weights_softmin_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction_loss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMove\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAttackStyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAttackTargetPos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m              \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m          \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m          \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcustom_reward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m          \u001b[49m\u001b[43magent_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSavingCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\patry\\Documents\\GitHub\\nmmo\\implementations\\train_ppo.py:73\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[1;34m(env, agent, episodes, print_every, save_every, eval_every, eval_episodes, agent_name, start_episode, custom_reward, callbacks)\u001b[0m\n\u001b[0;32m     70\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m custom_reward\u001b[38;5;241m.\u001b[39mget_rewards(step, observations, rewards, terminations, truncations)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m---> 73\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m     76\u001b[0m     episode_data[agent_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(observations[agent_id])\n",
      "File \u001b[1;32mc:\\Users\\patry\\Documents\\GitHub\\nmmo\\implementations\\SavingCallback.py:65\u001b[0m, in \u001b[0;36mSavingCallback.step\u001b[1;34m(self, observations_per_agent, actions_per_agent, episode, step)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     59\u001b[0m     observations_per_agent: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     step: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m     63\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverified_existing:\n\u001b[1;32m---> 65\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_existing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverified_existing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rewards()\n",
      "File \u001b[1;32mc:\\Users\\patry\\Documents\\GitHub\\nmmo\\implementations\\SavingCallback.py:44\u001b[0m, in \u001b[0;36mSavingCallback._check_existing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjar\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverwrite:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSave with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_to_existing:\n",
      "\u001b[1;31mValueError\u001b[0m: Save with name 'curriculum_reward_30' already exists"
     ]
    }
   ],
   "source": [
    "agent_name = \"curriculum_reward_31\"\n",
    "save_name = agent_name\n",
    "\n",
    "train_ppo(nmmo.Env(conf),\n",
    "          SimplierInputAgentV2(\n",
    "            learning_rate=5e-5,\n",
    "            lr_decay=0.999,\n",
    "            min_lr=5e-7,\n",
    "            critic_learning_rate=5e-6,\n",
    "            critic_lr_decay=0.999,\n",
    "            critic_min_lr=5e-7,\n",
    "            epsilon=0.1,\n",
    "            epochs=50,\n",
    "            batch_size=256,\n",
    "            entropy_loss_coef=3e-5,\n",
    "            max_grad_norm=0.5,\n",
    "            sample_weights_softmin_temp=-0.5,\n",
    "            action_loss_weights={\n",
    "                \"Move\": 2.5,\n",
    "                \"AttackStyle\": 0.5,\n",
    "                \"AttackTargetPos\": 0.5,\n",
    "              }),\n",
    "          episodes=400,\n",
    "          save_every=25,\n",
    "          print_every=5,\n",
    "          eval_every=50,\n",
    "          eval_episodes=5,\n",
    "          custom_reward=reward,\n",
    "          agent_name=agent_name,\n",
    "          callbacks=[\n",
    "            SavingCallback(save_name, reward_config=reward.get_config())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_from_save(save_name, random_agent_reward=random_reward, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lifetimes_from_save(save_name, random_agent_lifetime=random_lifetime, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_from_save(save_name, window=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entropies_from_save(save_name, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_callback = PathTrackingCallback()\n",
    "\n",
    "evaluate_agent(\n",
    "    nmmo.Env(conf),\n",
    "    SimplierInputAgentV2.load(f\"{agent_name}_best\"),\n",
    "    episodes=10,\n",
    "    custom_reward=LifetimeReward(1024),\n",
    "    callbacks=[\n",
    "        path_callback,\n",
    "        AnimationCallback(1, f\"{agent_name}_best_animation\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_callback.plot_paths(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_observations(save_name: str) -> None:\n",
    "    observations = get_all_observations_from_save(save_name, agent_ids=list(range(1, 33)))\n",
    "    net_inputs = [observations_to_inputs_simplier(obs, device=\"cpu\") for obs in observations]\n",
    "\n",
    "    tiles = [inp[0][0] for inp in net_inputs]\n",
    "    tile_features = [feature for tile in tiles for feature in tile.reshape(-1, 28)[:, -9:]if not torch.all(feature == 0)]\n",
    "\n",
    "    self_datas = [inp[1][0] for inp in net_inputs]\n",
    "    move_masks = [inp[2][0] for inp in net_inputs]\n",
    "    attack_masks = [inp[3][0] for inp in net_inputs]\n",
    "\n",
    "    def assert_for_all(values, assertion_fn, description):\n",
    "        correct_count = sum([assertion_fn(tensor) for tensor in values])\n",
    "        total_count = len(values)\n",
    "        print(f\"{(description+':'):<35}{correct_count}/{total_count} {('✅' if correct_count == total_count else '❌')}\")\n",
    "\n",
    "    assert_for_all(tiles, lambda x: x.shape == torch.Size([15, 15, 28]), \"Tiles shape\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, :16], dim=-1) == 1), \"16 features one-hot encoded\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, 16:18], dim=-1) == 1), \"Each tile either passable or not\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.logical_or(x[:, :, 18] == 0, x[:, :, 18] == 1)), \"Each tile harvestable or not\")\n",
    "    # TODO: Check seen entity data\n",
    "    print()\n",
    "\n",
    "    assert_for_all(self_datas, lambda x: x.shape == torch.Size([5]), \"Self data shape\")\n",
    "    assert_for_all(self_datas, lambda x: torch.all((x >= 0) & (x <= 1)), \"All values between 0 and 1\")\n",
    "    print()\n",
    "\n",
    "    assert_for_all(attack_masks, lambda x: x.shape == torch.Size([3]), \"Attack mask shape\")\n",
    "    assert_for_all(attack_masks, lambda x: torch.all(x == 1), \"Every attack style valid\")\n",
    "    print()\n",
    "\n",
    "    assert_for_all(move_masks, lambda x: x.shape == torch.Size([5]), \"Move mask shape\")\n",
    "    assert_for_all(move_masks, lambda x: x[-1] == 1, \"Can not move\")\n",
    "    assert_for_all(move_masks, lambda x: torch.any(x[:-1] == 1), \"Can move somewhere\")\n",
    "\n",
    "    seen_ids = {}\n",
    "    for obs in observations:\n",
    "        if obs.agent_id not in seen_ids:\n",
    "            seen_ids[obs.agent_id] = {}\n",
    "            \n",
    "        for seen_id in obs.entities.id:\n",
    "            if seen_id == 0 or seen_id == obs.agent_id:\n",
    "                continue\n",
    "            \n",
    "            if seen_id not in seen_ids[obs.agent_id]:\n",
    "                seen_ids[obs.agent_id][seen_id] = 0\n",
    "                \n",
    "            seen_ids[obs.agent_id][seen_id] += 1\n",
    "            \n",
    "    # Check that if one agent sees another, the other agent sees the first agent\n",
    "    for agent_id, seen in seen_ids.items():\n",
    "        for seen_id, count in seen.items():\n",
    "            if seen_ids.get(seen_id, {}).get(agent_id, 0) != count:\n",
    "                print(f\"Agent {agent_id} saw {seen_id} {count} times, but {seen_id} saw {agent_id} {seen_ids.get(seen_id, {}).get(agent_id, 0)} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
