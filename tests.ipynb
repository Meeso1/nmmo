{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nmmo\n",
    "from nmmo import config\n",
    "\n",
    "from implementations.train_ppo import train_ppo, evaluate_agent\n",
    "from implementations.SimplierInputAgentV2 import SimplierInputAgentV2\n",
    "from implementations.RandomAgent import get_avg_lifetime_for_random_agent, get_avg_reward_for_random_agent\n",
    "from implementations.Observations import Observations\n",
    "from implementations.CustomRewardBase import LifetimeReward, \\\n",
    "    ResourcesAndGatheringReward, ExplorationReward, WeightedReward, ShiftingReward\n",
    "from implementations.StayNearResourcesReward import StayNearResourcesReward\n",
    "from implementations.SavingCallback import SavingCallback\n",
    "from implementations.AnimationCallback import AnimationCallback\n",
    "from implementations.PathTrackingCallback import PathTrackingCallback\n",
    "from implementations.SpawnTrackingCallback import SpawnTrackingCallback\n",
    "from implementations.observations_to_inputs import observations_to_inputs_simplier\n",
    "from implementations.plots import *\n",
    "from implementations.jar import Jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config.Default()\n",
    "conf.set(\"PLAYER_N\", 32)\n",
    "#conf.set(\"NPC_N\", 0)\n",
    "\n",
    "reward = WeightedReward({\n",
    "    StayNearResourcesReward(1024, target_distance=2): 1,\n",
    "    ResourcesAndGatheringReward(1024, gathering_bonus=4, scale_with_resource_change=False): 1,\n",
    "    ExplorationReward(1024): 0.2\n",
    "})\n",
    "\n",
    "random_reward, random_rewards = get_avg_reward_for_random_agent(conf, reward=reward, retries=20)\n",
    "random_reward_std = np.std(random_rewards)\n",
    "print(f\"Random agent reward: {random_reward:.6f} ± {random_reward_std:.6f}\")\n",
    "\n",
    "random_lifetime, random_lifetimes = get_avg_lifetime_for_random_agent(conf, retries=20)\n",
    "random_lifetime_std = np.std(random_lifetimes)\n",
    "print(f\"Random agent lifetime: {random_lifetime:4.2f} ± {random_lifetime_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"compound_reward_retry_negative_sample_weights\"\n",
    "save_name = agent_name\n",
    "\n",
    "train_ppo(nmmo.Env(conf),\n",
    "          SimplierInputAgentV2(\n",
    "            learning_rate=5e-5,\n",
    "            lr_decay=0.999,\n",
    "            min_lr=5e-7,\n",
    "            critic_learning_rate=5e-6,\n",
    "            critic_lr_decay=0.999,\n",
    "            critic_min_lr=5e-7,\n",
    "            epsilon=0.1,\n",
    "            epochs=50,\n",
    "            batch_size=256,\n",
    "            entropy_loss_coef=3e-5,\n",
    "            max_grad_norm=0.5,\n",
    "            sample_weights_softmin_temp=-1,\n",
    "            action_loss_weights={\n",
    "                \"Move\": 2.5,\n",
    "                \"AttackStyle\": 0.5,\n",
    "                \"AttackTargetPos\": 0.5,\n",
    "              }),\n",
    "          episodes=3000,\n",
    "          save_every=25,\n",
    "          print_every=5,\n",
    "          eval_every=25,\n",
    "          eval_episodes=5,\n",
    "          custom_reward=reward,\n",
    "          agent_name=agent_name,\n",
    "          callbacks=[\n",
    "            SavingCallback(save_name, reward_config=reward.get_config())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_from_save(save_name, random_agent_reward=random_reward, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lifetimes_from_save(save_name, random_agent_lifetime=random_lifetime, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_from_save(save_name, window=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entropies_from_save(save_name, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spawn_callback = SpawnTrackingCallback()\n",
    "\n",
    "_ = evaluate_agent(\n",
    "    nmmo.Env(conf),\n",
    "    SimplierInputAgentV2.load(f\"{agent_name}_best\"),\n",
    "    episodes=10,\n",
    "    custom_reward=LifetimeReward(1024),\n",
    "    callbacks=[\n",
    "        spawn_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spawn_callback.plot_density_reward_correlation(max_distance=50, smoothing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_observations_from_save(save_name: str, agent_ids: list[int]) -> list[Observations]:       \n",
    "    history = Jar(\"saves\").get(save_name)\n",
    "    observations = [ep_obs[agent_id] \n",
    "                   for ep in history \n",
    "                   for ep_obs, _ in ep[0] \n",
    "                   for agent_id in agent_ids\n",
    "                   if agent_id in ep_obs]\n",
    "    return observations\n",
    "\n",
    "def verify_observations(save_name: str) -> None:\n",
    "    observations = get_all_observations_from_save(save_name, agent_ids=list(range(1, 33)))\n",
    "    net_inputs = [observations_to_inputs_simplier(obs, device=\"cpu\") for obs in observations]\n",
    "\n",
    "    tiles = [inp[0][0] for inp in net_inputs]\n",
    "    tile_features = [feature for tile in tiles for feature in tile.reshape(-1, 28)[:, -9:]if not torch.all(feature == 0)]\n",
    "\n",
    "    self_datas = [inp[1][0] for inp in net_inputs]\n",
    "    move_masks = [inp[2][0] for inp in net_inputs]\n",
    "    attack_masks = [inp[3][0] for inp in net_inputs]\n",
    "\n",
    "    def assert_for_all(values, assertion_fn, description):\n",
    "        correct_count = sum([assertion_fn(tensor) for tensor in values])\n",
    "        total_count = len(values)\n",
    "        print(f\"{(description+':'):<35}{correct_count}/{total_count} {('✅' if correct_count == total_count else '❌')}\")\n",
    "\n",
    "    assert_for_all(tiles, lambda x: x.shape == torch.Size([15, 15, 28]), \"Tiles shape\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, :16], dim=-1) == 1), \"16 features one-hot encoded\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, 16:18], dim=-1) == 1), \"Each tile either passable or not\")\n",
    "    assert_for_all(tiles, lambda x: torch.all(torch.logical_or(x[:, :, 18] == 0, x[:, :, 18] == 1)), \"Each tile harvestable or not\")\n",
    "    # TODO: Check seen entity data\n",
    "    print()\n",
    "\n",
    "    assert_for_all(self_datas, lambda x: x.shape == torch.Size([5]), \"Self data shape\")\n",
    "    assert_for_all(self_datas, lambda x: torch.all((x >= 0) & (x <= 1)), \"All values between 0 and 1\")\n",
    "    print()\n",
    "\n",
    "    assert_for_all(attack_masks, lambda x: x.shape == torch.Size([3]), \"Attack mask shape\")\n",
    "    assert_for_all(attack_masks, lambda x: torch.all(x == 1), \"Every attack style valid\")\n",
    "    print()\n",
    "\n",
    "    assert_for_all(move_masks, lambda x: x.shape == torch.Size([5]), \"Move mask shape\")\n",
    "    assert_for_all(move_masks, lambda x: x[-1] == 1, \"Can not move\")\n",
    "    assert_for_all(move_masks, lambda x: torch.any(x[:-1] == 1), \"Can move somewhere\")\n",
    "\n",
    "    seen_ids = {}\n",
    "    for obs in observations:\n",
    "        if obs.agent_id not in seen_ids:\n",
    "            seen_ids[obs.agent_id] = {}\n",
    "            \n",
    "        for seen_id in obs.entities.id:\n",
    "            if seen_id == 0 or seen_id == obs.agent_id:\n",
    "                continue\n",
    "            \n",
    "            if seen_id not in seen_ids[obs.agent_id]:\n",
    "                seen_ids[obs.agent_id][seen_id] = 0\n",
    "                \n",
    "            seen_ids[obs.agent_id][seen_id] += 1\n",
    "            \n",
    "    # Check that if one agent sees another, the other agent sees the first agent\n",
    "    for agent_id, seen in seen_ids.items():\n",
    "        for seen_id, count in seen.items():\n",
    "            if seen_ids.get(seen_id, {}).get(agent_id, 0) != count:\n",
    "                print(f\"Agent {agent_id} saw {seen_id} {count} times, but {seen_id} saw {agent_id} {seen_ids.get(seen_id, {}).get(agent_id, 0)} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
