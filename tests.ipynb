{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmmo\n",
    "from implementations.train_ppo import train_ppo, EvaluationCallback, evaluate_agent\n",
    "from implementations.PpoAgent import PPOAgent\n",
    "from implementations.Observations import Observations\n",
    "from implementations.CustomRewardBase import LifetimeReward, ResourcesReward\n",
    "from implementations.SavingCallback import SavingCallback\n",
    "from implementations.jar import Jar\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from nmmo import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimationCallback(EvaluationCallback):\n",
    "    def __init__(self, agent_id: int, output_name: str, quiet: bool = True):\n",
    "        self.agent_id = agent_id\n",
    "        self.output_name= output_name\n",
    "        self.quiet = quiet\n",
    "        self._plots_dir = \"plots\"\n",
    "        self._image_dir = f\"{self._plots_dir}/frames\"\n",
    "        self._current_episode_steps = 0\n",
    "    \n",
    "    def create_animation(self, output_file: str, fps: float = 2) -> None:\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        def update(step):\n",
    "            filename = os.path.join(self._image_dir, f\"step_{step}.png\")\n",
    "            if os.path.exists(filename):\n",
    "                img = plt.imread(filename)\n",
    "                ax.clear()\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                if not self.quiet:\n",
    "                    print(f\"File {filename} does not exist\")\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=self._current_episode_steps, repeat=False)\n",
    "        ani.save(output_file, writer='pillow', fps=fps)\n",
    "        plt.close(fig)\n",
    "        if not self.quiet:\n",
    "            print(f\"Saved animation to {output_file}\")\n",
    "\n",
    "    def plot_agent_view(self, obs: dict[int, Observations], env_actions: dict[int, dict[str, dict[str, int]]], agent_id: int, step: int) -> None:\n",
    "        if not os.path.exists(self._image_dir):\n",
    "            os.makedirs(self._image_dir)\n",
    "\n",
    "        agent_observations = obs.get(agent_id, None)\n",
    "        if agent_observations is None:\n",
    "            if not self.quiet:\n",
    "                print(f\"Agent {agent_id} not found in observations at step {step}\")\n",
    "            return\n",
    "\n",
    "        tiles = agent_observations.tiles\n",
    "        tile_rows, tile_cols, tile_values = tiles[:, 0], tiles[:, 1], tiles[:, 2]\n",
    "\n",
    "        agent_health = agent_observations.entities.health[0]\n",
    "        agent_food = agent_observations.entities.food[0]\n",
    "        agent_water = agent_observations.entities.water[0]\n",
    "\n",
    "        min_row, max_row = tile_rows.min(), tile_rows.max()\n",
    "        min_col, max_col = tile_cols.min(), tile_cols.max()\n",
    "        grid_rows, grid_cols = max_row - min_row + 1, max_col - min_col + 1\n",
    "        grid = np.zeros((grid_rows, grid_cols))\n",
    "\n",
    "        for r, c, v in zip(tile_rows, tile_cols, tile_values):\n",
    "            grid[r - min_row, c - min_col] = v\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.imshow(grid, cmap='coolwarm', interpolation='nearest', alpha=0.8)\n",
    "\n",
    "        def get_health_color(health):\n",
    "            if health > 75:\n",
    "                return 'green'\n",
    "            elif health > 50:\n",
    "                return 'yellow'\n",
    "            elif health > 25:\n",
    "                return 'orange'\n",
    "            else:\n",
    "                return 'red'\n",
    "\n",
    "        center_x, center_y = (\n",
    "            agent_observations.entities.row[0] - min_row,\n",
    "            agent_observations.entities.col[0] - min_col\n",
    "        )\n",
    "        agent_health_color = 'lightgreen' if agent_health > 75 else \\\n",
    "                            'yellowgreen' if agent_health > 50 else \\\n",
    "                            'darkorange' if agent_health > 25 else 'darkred'\n",
    "        ax.scatter(center_y, center_x, c=agent_health_color, s=100, label=f'Agent {agent_id}', edgecolors='black')\n",
    "\n",
    "        agent_stats = f\"Health: {agent_health}\\nFood: {agent_food}\\nWater: {agent_water}\"\n",
    "\n",
    "        action = env_actions.get(agent_id, None)\n",
    "        action_details = \"No action\"\n",
    "\n",
    "        if action:\n",
    "            move_direction = action.get('Move', {}).get('Direction', None)\n",
    "            attack_style = action.get('Attack', {}).get('Style', None)\n",
    "            attack_target = action.get('Attack', {}).get('Target', None)\n",
    "            use_item = action.get('Use', {}).get('InventoryItem', None)\n",
    "            destroy_item = action.get('Destroy', {}).get('InventoryItem', None)\n",
    "\n",
    "            action_details = \"\"\n",
    "            if move_direction is not None:\n",
    "                action_details += f\"Move: Direction {move_direction}\\n\"\n",
    "            if attack_style is not None and attack_target is not None:\n",
    "                action_details += f\"Attack: Style {attack_style}, Target {attack_target}\\n\"\n",
    "            if use_item is not None:\n",
    "                action_details += f\"Use: Item {use_item}\\n\"\n",
    "            if destroy_item is not None:\n",
    "                action_details += f\"Destroy: Item {destroy_item}\\n\"\n",
    "\n",
    "        ax.text(0.05, 0.05, f\"Action:\\n{action_details}\", transform=ax.transAxes, fontsize=10, \n",
    "                verticalalignment='bottom', horizontalalignment='left', color='black', \n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='black'))\n",
    "\n",
    "        for idx, entity_id in enumerate(agent_observations.entities.id):\n",
    "            if entity_id == agent_id or entity_id == 0:\n",
    "                continue\n",
    "\n",
    "            entity_row = agent_observations.entities.row[idx]\n",
    "            entity_col = agent_observations.entities.col[idx]\n",
    "            entity_health = agent_observations.entities.health[idx]\n",
    "\n",
    "            local_x = entity_row - min_row\n",
    "            local_y = entity_col - min_col\n",
    "            ax.scatter(\n",
    "                local_y, local_x, c=get_health_color(entity_health), s=50, alpha=0.7,\n",
    "                label=f'Entity {entity_id}', edgecolors='black'\n",
    "            )\n",
    "\n",
    "        ax.set_xticks(np.arange(grid_cols))\n",
    "        ax.set_yticks(np.arange(grid_rows))\n",
    "        ax.set_xticklabels(np.arange(min_col, max_col + 1))\n",
    "        ax.set_yticklabels(np.arange(min_row, max_row + 1))\n",
    "\n",
    "        ax.set_title(f\"Agent {agent_id}'s View at Step {step}\")\n",
    "        ax.text(0.95, 0.95, agent_stats, transform=ax.transAxes, fontsize=12, \n",
    "                verticalalignment='top', horizontalalignment='right', color='black', \n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='black'))\n",
    "        ax.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        output_file = f\"{self._image_dir}/step_{step}.png\"\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "        if not self.quiet:\n",
    "            print(f\"Saved agent view to {output_file}\")\n",
    "        \n",
    "    def step(\n",
    "        self,\n",
    "        observations_per_agent: dict[int, Observations], \n",
    "        actions_per_agent: dict[int, dict[str, dict[str, int]]], \n",
    "        episode: int, \n",
    "        step: int) -> None:\n",
    "        self.plot_agent_view(observations_per_agent, actions_per_agent, self.agent_id, step)\n",
    "        self._current_episode_steps += 1\n",
    "\n",
    "    def episode_start(self, episode: int) -> None:\n",
    "        self._current_episode_steps = 0\n",
    "\n",
    "    def episode_end(\n",
    "        self, \n",
    "        episode: int, \n",
    "        rewards_per_agent: dict[int, float], \n",
    "        losses: tuple[list[float], list[float], list[float]]\n",
    "    )-> None:\n",
    "        self.create_animation(f\"{self._plots_dir}/animations/{self.output_name}_{episode}_{int(time.time())}.gif\", fps=2)\n",
    "        if os.path.exists(self._image_dir):\n",
    "            shutil.rmtree(self._image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config.Default()\n",
    "conf.set(\"PLAYER_N\", 32)\n",
    "conf.set(\"NPC_N\", 0)\n",
    "\n",
    "train_ppo(nmmo.Env(conf),\n",
    "          episodes=200, \n",
    "          save_every=200,\n",
    "          print_every=10,\n",
    "          custom_reward=ResourcesReward(1024),\n",
    "          #custom_reward=LifetimeReward(1024),\n",
    "          agent_name=\"reward_for_gathering_32_agents\",\n",
    "          callbacks=[SavingCallback(\"reward_for_gathering_32_agents\", saved_agent_ids=[])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(actor_losses: list[float], critic_losses: list[float], total_losses: list[float]):\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(actor_losses, label=\"Actor Loss\", color='blue')\n",
    "    ax.plot(critic_losses, label=\"Critic Loss\", color='red')\n",
    "    ax.plot(total_losses, label=\"Total Loss\", color='green')\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Losses Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_losses_from_save(agent_name: str):\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    losses = [episode[2] for episode in history]\n",
    "    actor_losses = [l for loss in losses for l in loss[0]]\n",
    "    critic_losses = [l for loss in losses for l in loss[1]]\n",
    "    total_losses = [l for loss in losses for l in loss[2]]\n",
    "    \n",
    "    plot_losses(actor_losses, critic_losses, total_losses)\n",
    "\n",
    "\n",
    "def plot_rewards(avg_rewards, max_rewards, min_rewards, ninetieth_percentile_rewards, window=50):\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(ninetieth_percentile_rewards, label=\"90th Percentile Reward\", color='purple')\n",
    "    ax.plot(avg_rewards, label=\"Average Reward\", color='blue')\n",
    "    ax.plot(max_rewards, label=\"Max Reward\", color='red')\n",
    "    ax.plot(min_rewards, label=\"Min Reward\", color='green')\n",
    "\n",
    "    if window > 1:\n",
    "        avg_rewards_smooth = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(avg_rewards)), avg_rewards_smooth, label=f\"Running Mean (window={window})\", color='orange')\n",
    "\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    ax.set_title(\"Rewards Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_rewards_from_save(agent_name: str, window: int = 50):\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    rewards = [episode[1] for episode in history]\n",
    "    avg_rewards = [np.mean([r for r in reward.values()]) for reward in rewards]\n",
    "    max_rewards = [np.max([r for r in reward.values()]) for reward in rewards]\n",
    "    min_rewards = [np.min([r for r in reward.values()]) for reward in rewards]\n",
    "    ninetieth_percentile_rewards = [np.percentile([r for r in reward.values()], 90) for reward in rewards]\n",
    "    \n",
    "    plot_rewards(avg_rewards, max_rewards, min_rewards, ninetieth_percentile_rewards, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_from_save(\"reward_for_gathering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
