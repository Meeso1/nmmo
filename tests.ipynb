{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmmo\n",
    "from implementations.train_ppo import train_ppo, EvaluationCallback, evaluate_agent\n",
    "from implementations.PpoAgent import PPOAgent\n",
    "from implementations.SimplierInputAgent import SimplierInputAgent\n",
    "from implementations.SimplierInputAgentV2 import SimplierInputAgentV2\n",
    "from implementations.RandomAgent import get_avg_lifetime_for_random_agent, get_avg_reward_for_random_agent\n",
    "from implementations.Observations import Observations\n",
    "from implementations.CustomRewardBase import LifetimeReward, ResourcesReward, CustomRewardBase, ResourcesAndGatheringReward\n",
    "from implementations.SavingCallback import SavingCallback\n",
    "from implementations.AnimationCallback import AnimationCallback\n",
    "from implementations.observations_to_inputs import observations_to_inputs_simplier\n",
    "from implementations.jar import Jar\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nmmo import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_observations_from_save(save_name: str, agent_ids: list[int]) -> list[Observations]:       \n",
    "    history = Jar(\"saves\").get(save_name)\n",
    "    observations = [ep_obs[agent_id] \n",
    "                   for ep in history \n",
    "                   for ep_obs, _ in ep[0] \n",
    "                   for agent_id in agent_ids\n",
    "                   if agent_id in ep_obs]\n",
    "    return observations\n",
    "\n",
    "def get_entropies_from_save(save_name: str) -> dict[str, list[float]]:     \n",
    "\thistory = Jar(\"saves\").get(save_name)\n",
    "\tentropies_per_episode = [[entropies \n",
    "                           \t\tfor step in ep[4] \n",
    "                           \t\tfor entropies in step.values()]\n",
    "\t\t\t\t\t\t    for ep in history]\n",
    " \n",
    "\tmeans_per_episode_per_type = [{type: np.mean([entropies[type] for entropies in ep]) \n",
    "                                  \tfor type in ep[0].keys()} \n",
    "                                 for ep in entropies_per_episode]\n",
    " \n",
    "\tmeans_per_type_per_episode = {type: [ep[type] for ep in means_per_episode_per_type]\n",
    "                                  for type in means_per_episode_per_type[0].keys()}\n",
    "\treturn means_per_type_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(\n",
    "    actor_losses: list[float], \n",
    "    critic_losses: list[float],\n",
    "    window: int = 500\n",
    ") -> None:\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    ax1.plot(actor_losses, label=\"Actor Loss\", color='blue', alpha=0.4)\n",
    "    actor_losses_smooth = np.convolve(actor_losses, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    actor_losses_std = np.array([np.std(actor_losses[max(0, i-window):i+1]) \n",
    "                                for i in range(window-1, len(actor_losses))])\n",
    "    \n",
    "    ax1.plot(range(window-1, len(actor_losses)), actor_losses_smooth, \n",
    "             label=f\"Running Mean (window={window})\", color='red')\n",
    "    ax1.fill_between(range(window-1, len(actor_losses)), \n",
    "                     actor_losses_smooth - actor_losses_std,\n",
    "                     actor_losses_smooth + actor_losses_std,\n",
    "                     alpha=0.2, color='red', label='Standard Deviation')\n",
    "    \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Actor Loss Over Time\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.semilogy(critic_losses, label=\"Critic Loss\", color='blue', alpha=0.4)\n",
    "    critic_losses_smooth = np.convolve(critic_losses, np.ones(window)/window, mode='valid')\n",
    "    ax2.semilogy(range(window-1, len(critic_losses)), critic_losses_smooth, \n",
    "                 label=f\"Running Mean (window={window})\", color='red')\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Loss (log scale)\")\n",
    "    ax2.set_title(\"Critic Loss Over Time\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_losses_from_save(agent_name: str, window: int = 500) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    losses = [episode[2] for episode in history]\n",
    "    actor_losses = [l for loss in losses for l in loss[0]]\n",
    "    critic_losses = [l for loss in losses for l in loss[1]]\n",
    "    \n",
    "    plot_losses(actor_losses, critic_losses, window)\n",
    "\n",
    "def plot_rewards(\n",
    "    avg_rewards: list[float], \n",
    "    max_rewards: list[float], \n",
    "    min_rewards: list[float], \n",
    "    ninetieth_percentile_rewards: list[float] | None = None, \n",
    "    random_agent_reward: float | None = None,\n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if ninetieth_percentile_rewards is not None:\n",
    "        ax.plot(ninetieth_percentile_rewards, label=\"90th Percentile Reward\", color='purple', alpha=0.4)\n",
    "        \n",
    "    ax.plot(avg_rewards, label=\"Average Reward\", color='red', alpha=0.4)\n",
    "    ax.plot(max_rewards, label=\"Max Reward\", color='pink', alpha=0.8)\n",
    "    ax.plot(min_rewards, label=\"Min Reward\", color='green', alpha=0.4)\n",
    "    \n",
    "    if random_agent_reward is not None:\n",
    "        ax.axhline(y=random_agent_reward, label=\"Random Agent Reward\", color='black', linestyle='--')\n",
    "    \n",
    "    avg_rewards_smooth = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(avg_rewards)), avg_rewards_smooth, label=f\"Running Mean (window={window})\", color='blue')\n",
    "\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    ax.set_title(\"Rewards Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_rewards_from_save(agent_name: str, window: int = 50, random_agent_reward: float | None = None) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    rewards = [episode[1] for episode in history]\n",
    "    num_agents = len(rewards[0])\n",
    "    \n",
    "    avg_rewards = [np.mean([r for r in reward.values()]) for reward in rewards]\n",
    "    max_rewards = [np.max([r for r in reward.values()]) for reward in rewards]\n",
    "    min_rewards = [np.min([r for r in reward.values()]) for reward in rewards]\n",
    "    \n",
    "    if num_agents > 30:\n",
    "        ninetieth_percentile_rewards = [np.percentile([r for r in reward.values()], 90) for reward in rewards]\n",
    "    else:\n",
    "        ninetieth_percentile_rewards = None\n",
    "    \n",
    "    plot_rewards(avg_rewards, max_rewards, min_rewards, ninetieth_percentile_rewards, random_agent_reward, window)\n",
    "\n",
    "def plot_lifetimes(\n",
    "    avg_lifetimes: list[float], \n",
    "    max_lifetimes: list[float], \n",
    "    min_lifetimes: list[float], \n",
    "    ninetieth_percentile: list[float] | None = None,\n",
    "    random_agent_lifetime: float | None = None,\n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if ninetieth_percentile is not None:\n",
    "        ax.plot(ninetieth_percentile, label=\"90th Percentile\", color='purple', alpha=0.4)\n",
    "        \n",
    "    ax.plot(avg_lifetimes, label=\"Average Lifetime\", color='red', alpha=0.4)\n",
    "    ax.plot(max_lifetimes, label=\"Max Lifetime\", color='pink', alpha=0.8)\n",
    "    ax.plot(min_lifetimes, label=\"Min Lifetime\", color='green', alpha=0.4)\n",
    "    \n",
    "    if random_agent_lifetime is not None:\n",
    "        ax.axhline(y=random_agent_lifetime, label=\"Random Agent Lifetime\", color='black', linestyle='--')\n",
    "\n",
    "    avg_rewards_smooth = np.convolve(avg_lifetimes, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(avg_lifetimes)), avg_rewards_smooth, label=f\"Running Mean (window={window})\", color='blue')\n",
    "\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Lifetime\")\n",
    "    ax.set_title(\"Agent Lifetime Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()   \n",
    "\n",
    "def plot_lifetimes_from_save(agent_name: str, random_agent_lifetime: float | None = None, window: int = 50) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    lifetimes = [episode[3] for episode in history]\n",
    "    num_agents = len(lifetimes[0])\n",
    "    \n",
    "    avg_lifetimes = [np.mean([r for r in reward.values()]) for reward in lifetimes]\n",
    "    max_lifetimes = [np.max([r for r in reward.values()]) for reward in lifetimes]\n",
    "    min_lifetimes = [np.min([r for r in reward.values()]) for reward in lifetimes]\n",
    "    \n",
    "    if num_agents > 30:\n",
    "        ninetieth_percentile = [np.percentile([r for r in lifetime.values()], 90) for lifetime in lifetimes]\n",
    "    else:\n",
    "        ninetieth_percentile = None\n",
    "    \n",
    "    plot_lifetimes(avg_lifetimes, max_lifetimes, min_lifetimes, ninetieth_percentile, random_agent_lifetime, window)\n",
    "    \n",
    "def plot_entropies(\n",
    "    means_per_type_per_episode: dict[str, list[float]], \n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, axs = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for idx, (type, entropies) in enumerate(means_per_type_per_episode.items()):\n",
    "        ax = axs[idx]\n",
    "        ax.plot(entropies, label=f\"{type} Entropy\", alpha=0.4)\n",
    "        \n",
    "        entropies_smooth = np.convolve(entropies, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(entropies)), entropies_smooth, \n",
    "               label=f\"Running Mean (window={window})\")\n",
    "        \n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.set_ylabel(\"Entropy\")\n",
    "        ax.set_title(f\"{type} Entropy Over Time\")\n",
    "        ax.set_ylim(bottom=0)  # Set y-axis minimum to 0\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_entropies_from_save(agent_name: str, window: int = 50) -> None:\n",
    "    plot_entropies(get_entropies_from_save(agent_name), window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config.Default()\n",
    "conf.set(\"PLAYER_N\", 32)\n",
    "conf.set(\"NPC_N\", 0)\n",
    "\n",
    "reward = ResourcesAndGatheringReward(1024)\n",
    "\n",
    "random_reward, random_rewards = get_avg_reward_for_random_agent(conf, reward=reward, retries=20)\n",
    "random_reward_std = np.std(random_rewards)\n",
    "\n",
    "random_lifetime, random_lifetimes = get_avg_lifetime_for_random_agent(conf, retries=20)\n",
    "random_lifetime_std = np.std(random_lifetimes)\n",
    "\n",
    "print(f\"Random agent reward: {random_reward:.6f} ± {random_reward_std:.6f}\")\n",
    "print(f\"Random agent lifetime: {random_lifetime:4.2f} ± {random_lifetime_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"observations_verification_quick_run_4\"\n",
    "save_name = agent_name\n",
    "\n",
    "train_ppo(nmmo.Env(conf),\n",
    "          SimplierInputAgentV2(\n",
    "            learning_rate=1e-4,\n",
    "            epsilon=0.1,\n",
    "            epochs=50,\n",
    "            batch_size=256,\n",
    "            entropy_loss_coef=1e-4,\n",
    "            action_loss_weights={\n",
    "                \"AttackStyle\": 0,\n",
    "                \"AttackOrNot\": 0,\n",
    "                \"AttackTargetPos\": 0\n",
    "              }),\n",
    "          episodes=20,\n",
    "          save_every=20,\n",
    "          print_every=1,\n",
    "          start_episode=1,\n",
    "          custom_reward=reward,\n",
    "          agent_name=agent_name,\n",
    "          callbacks=[\n",
    "            SavingCallback(save_name, saved_agent_ids=list(range(1, 33)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_from_save(save_name, random_agent_reward=random_reward, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lifetimes_from_save(save_name, random_agent_lifetime=random_lifetime, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_from_save(save_name, window=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entropies_from_save(save_name, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = get_all_observations_from_save(save_name, agent_ids=list(range(1, 33)))\n",
    "net_inputs = [observations_to_inputs_simplier(obs, device=\"cpu\") for obs in observations]\n",
    "\n",
    "tiles = [inp[0][0] for inp in net_inputs]\n",
    "tile_features = [feature for tile in tiles for feature in tile.reshape(-1, 28)[:, -9:]if not torch.all(feature == 0)]\n",
    "\n",
    "self_datas = [inp[1][0] for inp in net_inputs]\n",
    "move_masks = [inp[2][0] for inp in net_inputs]\n",
    "attack_masks = [inp[3][0] for inp in net_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_for_all(values, assertion_fn, description):\n",
    "    correct_count = sum([assertion_fn(tensor) for tensor in values])\n",
    "    total_count = len(values)\n",
    "    print(f\"{(description+':'):<35}{correct_count}/{total_count} {('✅' if correct_count == total_count else '❌')}\")\n",
    "\n",
    "assert_for_all(tiles, lambda x: x.shape == torch.Size([15, 15, 28]), \"Tiles shape\")\n",
    "assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, :16], dim=-1) == 1), \"16 features one-hot encoded\")\n",
    "assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, 16:18], dim=-1) == 1), \"Each tile either passable or not\")\n",
    "assert_for_all(tiles, lambda x: torch.all(torch.logical_or(x[:, :, 18] == 0, x[:, :, 18] == 1)), \"Each tile harvestable or not\")\n",
    "# TODO: Check seen entity data\n",
    "print()\n",
    "\n",
    "assert_for_all(self_datas, lambda x: x.shape == torch.Size([5]), \"Self data shape\")\n",
    "assert_for_all(self_datas, lambda x: torch.all((x >= 0) & (x <= 1)), \"All values between 0 and 1\")\n",
    "print()\n",
    "\n",
    "assert_for_all(attack_masks, lambda x: x.shape == torch.Size([3]), \"Attack mask shape\")\n",
    "assert_for_all(attack_masks, lambda x: torch.all(x == 1), \"Every attack style valid\")\n",
    "print()\n",
    "\n",
    "assert_for_all(move_masks, lambda x: x.shape == torch.Size([5]), \"Move mask shape\")\n",
    "assert_for_all(move_masks, lambda x: x[-1] == 1, \"Can not move\")\n",
    "assert_for_all(move_masks, lambda x: torch.any(x[:-1] == 1), \"Can move somewhere\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_ids = {}\n",
    "for obs in observations:\n",
    "    if obs.agent_id not in seen_ids:\n",
    "        seen_ids[obs.agent_id] = {}\n",
    "        \n",
    "    for seen_id in obs.entities.id:\n",
    "        if seen_id == 0 or seen_id == obs.agent_id:\n",
    "            continue\n",
    "        \n",
    "        if seen_id not in seen_ids[obs.agent_id]:\n",
    "            seen_ids[obs.agent_id][seen_id] = 0\n",
    "            \n",
    "        seen_ids[obs.agent_id][seen_id] += 1\n",
    "        \n",
    "# Check that if one agent sees another, the other agent sees the first agent\n",
    "for agent_id, seen in seen_ids.items():\n",
    "    for seen_id, count in seen.items():\n",
    "        if seen_ids.get(seen_id, {}).get(agent_id, 0) != count:\n",
    "            print(f\"Agent {agent_id} saw {seen_id} {count} times, but {seen_id} saw {agent_id} {seen_ids.get(seen_id, {}).get(agent_id, 0)} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
