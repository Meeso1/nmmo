{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmmo\n",
    "from implementations.train_ppo import train_ppo, EvaluationCallback, evaluate_agent\n",
    "from implementations.PpoAgent import PPOAgent\n",
    "from implementations.SimplierInputAgent import SimplierInputAgent\n",
    "from implementations.RandomAgent import get_avg_lifetime_for_random_agent, get_avg_reward_for_random_agent\n",
    "from implementations.Observations import Observations\n",
    "from implementations.CustomRewardBase import LifetimeReward, ResourcesReward, CustomRewardBase, ResourcesAndGatheringReward\n",
    "from implementations.SavingCallback import SavingCallback\n",
    "from implementations.observations_to_inputs import observations_to_inputs_simplier\n",
    "from implementations.jar import Jar\n",
    "from implementations.ActionData import ActionData\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from nmmo import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimationCallback(EvaluationCallback):\n",
    "    def __init__(self, agent_id: int, output_name: str, quiet: bool = True):\n",
    "        self.agent_id = agent_id\n",
    "        self.output_name = output_name\n",
    "        self.quiet = quiet\n",
    "        self._plots_dir = \"plots\"\n",
    "        self._image_dir = f\"{self._plots_dir}/frames\"\n",
    "        self._current_episode_steps = 0\n",
    "        \n",
    "        self.tile_color_map = {\n",
    "            'void': '#000000',      # Black\n",
    "            'water': '#4169E1',     # Royal blue\n",
    "            'grass': '#7CBA3B',     # Yellow-green\n",
    "            'stone': '#808080',     # Gray\n",
    "            \n",
    "            'ore': '#8B4513',       # Saddle brown\n",
    "            'slag': '#A0522D',      # Sienna\n",
    "            \n",
    "            'herb': '#00FF7F',      # Spring green\n",
    "            'weeds': '#556B2F',     # Dark olive green\n",
    "            \n",
    "            'foilage': '#90EE90',   # Light green\n",
    "            'scrub': '#3CB371',     # Medium sea green\n",
    "            \n",
    "            'crystal': '#B8860B',   # Dark goldenrod\n",
    "            'fragment': '#DEB887',  # Burlywood\n",
    "            \n",
    "            'tree': '#228B22',      # Forest green\n",
    "            'stump': '#8B4513',     # Saddle brown\n",
    "            \n",
    "            'fish': '#87CEEB',      # Sky blue\n",
    "            'ocean': '#000080',     # Navy blue\n",
    "        }\n",
    "\n",
    "    def create_animation(self, output_file: str, fps: float = 2) -> None:\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        def update(step):\n",
    "            filename = os.path.join(self._image_dir, f\"step_{step}.png\")\n",
    "            if os.path.exists(filename):\n",
    "                img = plt.imread(filename)\n",
    "                ax.clear()\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                if not self.quiet:\n",
    "                    print(f\"File {filename} does not exist\")\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=self._current_episode_steps, repeat=False)\n",
    "        ani.save(output_file, writer='pillow', fps=fps)\n",
    "        plt.close(fig)\n",
    "        if not self.quiet:\n",
    "            print(f\"Saved animation to {output_file}\")\n",
    "\n",
    "    def _get_color_grid(\n",
    "        self, \n",
    "        tile_rows: np.ndarray, \n",
    "        tile_cols: np.ndarray, \n",
    "        tile_values: np.ndarray, \n",
    "        min_row: int, \n",
    "        min_col: int, \n",
    "        grid_rows: int, \n",
    "        grid_cols: int\n",
    "    ) -> np.ndarray:        \n",
    "        def hex_to_rgb(hex_color):\n",
    "            hex_color = hex_color.lstrip('#')\n",
    "            return [int(hex_color[i:i+2], 16)/255.0 for i in (0, 2, 4)]\n",
    "\n",
    "        index_to_material = {m.index: m.tex for m in nmmo.material.All.materials}\n",
    "        color_grid = np.zeros((grid_rows, grid_cols, 3))\n",
    "        for r, c, v in zip(tile_rows, tile_cols, tile_values):\n",
    "            color_grid[r - min_row, c - min_col] = hex_to_rgb(self.tile_color_map[index_to_material[v]])\n",
    "        return color_grid\n",
    "\n",
    "    def _get_action_text(self, action: dict[str, dict[str, int]]) -> str:\n",
    "        if not action:\n",
    "            return \"No action\"\n",
    "        \n",
    "        action_parts = []\n",
    "        if 'Move' in action and 'Direction' in action['Move']:\n",
    "            action_parts.append(f\"Move: Direction {action['Move']['Direction']}\")\n",
    "        if 'Attack' in action:\n",
    "            attack = action['Attack']\n",
    "            if 'Style' in attack and 'Target' in attack:\n",
    "                action_parts.append(f\"Attack: Style {attack['Style']}, Target {attack['Target']}\")\n",
    "        if 'Use' in action and 'InventoryItem' in action['Use']:\n",
    "            action_parts.append(f\"Use: Item {action['Use']['InventoryItem']}\")\n",
    "        if 'Destroy' in action and 'InventoryItem' in action['Destroy']:\n",
    "            action_parts.append(f\"Destroy: Item {action['Destroy']['InventoryItem']}\")\n",
    "            \n",
    "        return '\\n'.join(action_parts) if action_parts else \"No action\"\n",
    "\n",
    "    def _plot_entities(self, ax, agent_observations: Observations, min_row: int, min_col: int):\n",
    "        def get_health_color(health):\n",
    "            if health > 75: return 'green'\n",
    "            elif health > 50: return 'yellow'\n",
    "            elif health > 25: return 'orange'\n",
    "            return 'red'\n",
    "\n",
    "        for idx, entity_id in enumerate(agent_observations.entities.id):\n",
    "            if entity_id == self.agent_id or entity_id == 0:\n",
    "                continue\n",
    "\n",
    "            local_x = agent_observations.entities.row[idx] - min_row\n",
    "            local_y = agent_observations.entities.col[idx] - min_col\n",
    "            health = agent_observations.entities.health[idx]\n",
    "            ax.scatter(local_y, local_x, c=get_health_color(health), s=50, \n",
    "                      alpha=0.7, label=f'Entity {entity_id}', edgecolors='black')\n",
    "            \n",
    "    def _plot_tile_legend(self, ax):\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, len(self.tile_color_map))\n",
    "        ax.invert_yaxis()\n",
    "        ax.axis('off')\n",
    "        items = list(self.tile_color_map.items())\n",
    "        for idx, (mat, color) in enumerate(items):\n",
    "            rect = plt.Rectangle((0, idx), 0.5, 0.5, facecolor=color, edgecolor='black', alpha=0.8)\n",
    "            ax.add_patch(rect)\n",
    "            text_color = 'white' if mat in ['ocean', 'void'] else 'black'\n",
    "            ax.text(0.25, idx + 0.25, mat, va='center', ha='center', fontsize=12, family='monospace', color=text_color)\n",
    "\n",
    "    def plot_agent_view(\n",
    "        self, \n",
    "        obs: dict[int, Observations], \n",
    "        env_actions: dict[int, dict[str, dict[str, int]]], \n",
    "        agent_id: int, \n",
    "        step: int\n",
    "    ) -> None:\n",
    "        if not os.path.exists(self._image_dir):\n",
    "            os.makedirs(self._image_dir)\n",
    "\n",
    "        agent_observations = obs.get(agent_id)\n",
    "        if agent_observations is None or not isinstance(agent_observations, Observations):\n",
    "            if not self.quiet:\n",
    "                print(f\"Agent {agent_id} not found in observations at step {step}\")\n",
    "            return\n",
    "\n",
    "        # Extract tile information\n",
    "        tiles = agent_observations.tiles\n",
    "        tile_rows, tile_cols, tile_values = tiles[:, 0], tiles[:, 1], tiles[:, 2]\n",
    "        min_row, max_row = tile_rows.min(), tile_rows.max()\n",
    "        min_col, max_col = tile_cols.min(), tile_cols.max()\n",
    "        grid_rows, grid_cols = max_row - min_row + 1, max_col - min_col + 1\n",
    "\n",
    "        # Create plot\n",
    "        _, (ax, ax_legend) = plt.subplots(1, 2, figsize=(10, 8), gridspec_kw={'width_ratios': [10, 2]})\n",
    "        self._plot_tile_legend(ax_legend)\n",
    "        \n",
    "        color_grid = self._get_color_grid(tile_rows, tile_cols, tile_values, min_row, min_col, grid_rows, grid_cols)\n",
    "        ax.imshow(color_grid, interpolation='nearest', alpha=0.8)\n",
    "\n",
    "        agent_idx = np.where(agent_observations.entities.id == agent_id)[0][0]\n",
    "\n",
    "        # Plot agent\n",
    "        center_x = agent_observations.entities.row[agent_idx] - min_row\n",
    "        center_y = agent_observations.entities.col[agent_idx] - min_col\n",
    "        agent_health = agent_observations.entities.health[agent_idx]\n",
    "        agent_food = agent_observations.entities.food[agent_idx]\n",
    "        agent_water = agent_observations.entities.water[agent_idx]\n",
    "        agent_health_color = 'lightgreen' if agent_health > 75 else 'yellowgreen' if agent_health > 50 else 'darkorange' if agent_health > 25 else 'darkred'\n",
    "        ax.scatter(center_y, center_x, c=agent_health_color, s=100, label=f'Agent {agent_id}', edgecolors='black')\n",
    "\n",
    "        # Plot other entities\n",
    "        self._plot_entities(ax, agent_observations, min_row, min_col)\n",
    "\n",
    "        # Add text information\n",
    "        agent_stats = f\"Health: {agent_health:>3}\\n\" + \\\n",
    "                      f\"Food:   {agent_food:>3}\\n\" + \\\n",
    "                      f\"Water:  {agent_water:>3}\"\n",
    "        action_text = self._get_action_text(env_actions.get(agent_id))\n",
    "        \n",
    "        ax.text(0.05, 0.05, f\"Action:\\n{action_text}\", transform=ax.transAxes, fontsize=12,\n",
    "                verticalalignment='bottom', horizontalalignment='left', color='black',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='black'), family='monospace')\n",
    "        ax.text(0.95, 0.95, f\"Step:  {step:>4}\", transform=ax.transAxes, fontsize=12,\n",
    "                verticalalignment='top', horizontalalignment='right', color='black',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='black'), family='monospace')\n",
    "        ax.text(0.95, 0.90, agent_stats, transform=ax.transAxes, fontsize=12,\n",
    "                verticalalignment='top', horizontalalignment='right', color='black',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='black'), family='monospace')\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_xticks(np.arange(grid_cols))\n",
    "        ax.set_yticks(np.arange(grid_rows))\n",
    "        ax.set_xticklabels(np.arange(min_col, max_col + 1))\n",
    "        ax.set_yticklabels(np.arange(min_row, max_row + 1))\n",
    "        legend = ax.legend(loc='upper left')\n",
    "        for text in legend.get_texts():\n",
    "            text.set_family('monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_file = f\"{self._image_dir}/step_{step}.png\"\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "        \n",
    "        if not self.quiet:\n",
    "            print(f\"Saved agent view to {output_file}\")\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        observations_per_agent: dict[int, Observations], \n",
    "        actions_per_agent: dict[int, ActionData], \n",
    "        episode: int, \n",
    "        step: int) -> None:\n",
    "        self.plot_agent_view(\n",
    "            observations_per_agent, \n",
    "            {agent_id: actions.action_dict for agent_id, actions in actions_per_agent.items()}, \n",
    "            self.agent_id, \n",
    "            step)\n",
    "        self._current_episode_steps += 1\n",
    "\n",
    "    def episode_start(self, episode: int) -> None:\n",
    "        self._current_episode_steps = 0\n",
    "\n",
    "    def episode_end(\n",
    "        self, \n",
    "        episode: int, \n",
    "        rewards_per_agent: dict[int, float], \n",
    "        losses: tuple[list[float], list[float], list[float]]\n",
    "    )-> None:\n",
    "        self.create_animation(f\"{self._plots_dir}/animations/{self.output_name}_{episode}_{int(time.time())}.gif\", fps=2)\n",
    "        if os.path.exists(self._image_dir):\n",
    "            shutil.rmtree(self._image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(\n",
    "    actor_losses: list[float], \n",
    "    critic_losses: list[float],\n",
    "    window: int = 500\n",
    ") -> None:\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    ax1.plot(actor_losses, label=\"Actor Loss\", color='blue', alpha=0.4)\n",
    "    actor_losses_smooth = np.convolve(actor_losses, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    actor_losses_std = np.array([np.std(actor_losses[max(0, i-window):i+1]) \n",
    "                                for i in range(window-1, len(actor_losses))])\n",
    "    \n",
    "    ax1.plot(range(window-1, len(actor_losses)), actor_losses_smooth, \n",
    "             label=f\"Running Mean (window={window})\", color='red')\n",
    "    ax1.fill_between(range(window-1, len(actor_losses)), \n",
    "                     actor_losses_smooth - actor_losses_std,\n",
    "                     actor_losses_smooth + actor_losses_std,\n",
    "                     alpha=0.2, color='red', label='Standard Deviation')\n",
    "    \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Actor Loss Over Time\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.semilogy(critic_losses, label=\"Critic Loss\", color='blue', alpha=0.4)\n",
    "    critic_losses_smooth = np.convolve(critic_losses, np.ones(window)/window, mode='valid')\n",
    "    ax2.semilogy(range(window-1, len(critic_losses)), critic_losses_smooth, \n",
    "                 label=f\"Running Mean (window={window})\", color='red')\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Loss (log scale)\")\n",
    "    ax2.set_title(\"Critic Loss Over Time\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_losses_from_save(agent_name: str, window: int = 500) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    losses = [episode[2] for episode in history]\n",
    "    actor_losses = [l for loss in losses for l in loss[0]]\n",
    "    critic_losses = [l for loss in losses for l in loss[1]]\n",
    "    \n",
    "    plot_losses(actor_losses, critic_losses, window)\n",
    "\n",
    "def plot_rewards(\n",
    "    avg_rewards: list[float], \n",
    "    max_rewards: list[float], \n",
    "    min_rewards: list[float], \n",
    "    ninetieth_percentile_rewards: list[float] | None = None, \n",
    "    random_agent_reward: float | None = None,\n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if ninetieth_percentile_rewards is not None:\n",
    "        ax.plot(ninetieth_percentile_rewards, label=\"90th Percentile Reward\", color='purple', alpha=0.4)\n",
    "        \n",
    "    ax.plot(avg_rewards, label=\"Average Reward\", color='red', alpha=0.4)\n",
    "    ax.plot(max_rewards, label=\"Max Reward\", color='pink', alpha=0.8)\n",
    "    ax.plot(min_rewards, label=\"Min Reward\", color='green', alpha=0.4)\n",
    "    \n",
    "    if random_agent_reward is not None:\n",
    "        ax.axhline(y=random_agent_reward, label=\"Random Agent Reward\", color='black', linestyle='--')\n",
    "    \n",
    "    avg_rewards_smooth = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(avg_rewards)), avg_rewards_smooth, label=f\"Running Mean (window={window})\", color='blue')\n",
    "\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    ax.set_title(\"Rewards Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_rewards_from_save(agent_name: str, window: int = 50, random_agent_reward: float | None = None) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    rewards = [episode[1] for episode in history]\n",
    "    num_agents = len(rewards[0])\n",
    "    \n",
    "    avg_rewards = [np.mean([r for r in reward.values()]) for reward in rewards]\n",
    "    max_rewards = [np.max([r for r in reward.values()]) for reward in rewards]\n",
    "    min_rewards = [np.min([r for r in reward.values()]) for reward in rewards]\n",
    "    \n",
    "    if num_agents > 30:\n",
    "        ninetieth_percentile_rewards = [np.percentile([r for r in reward.values()], 90) for reward in rewards]\n",
    "    else:\n",
    "        ninetieth_percentile_rewards = None\n",
    "    \n",
    "    plot_rewards(avg_rewards, max_rewards, min_rewards, ninetieth_percentile_rewards, random_agent_reward, window)\n",
    "\n",
    "def plot_lifetimes(\n",
    "    avg_lifetimes: list[float], \n",
    "    max_lifetimes: list[float], \n",
    "    min_lifetimes: list[float], \n",
    "    ninetieth_percentile: list[float] | None = None,\n",
    "    random_agent_lifetime: float | None = None,\n",
    "    window: int = 50\n",
    ") -> None:\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    if ninetieth_percentile is not None:\n",
    "        ax.plot(ninetieth_percentile, label=\"90th Percentile\", color='purple', alpha=0.4)\n",
    "        \n",
    "    ax.plot(avg_lifetimes, label=\"Average Lifetime\", color='red', alpha=0.4)\n",
    "    ax.plot(max_lifetimes, label=\"Max Lifetime\", color='pink', alpha=0.8)\n",
    "    ax.plot(min_lifetimes, label=\"Min Lifetime\", color='green', alpha=0.4)\n",
    "    \n",
    "    if random_agent_lifetime is not None:\n",
    "        ax.axhline(y=random_agent_lifetime, label=\"Random Agent Lifetime\", color='black', linestyle='--')\n",
    "\n",
    "    avg_rewards_smooth = np.convolve(avg_lifetimes, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(avg_lifetimes)), avg_rewards_smooth, label=f\"Running Mean (window={window})\", color='blue')\n",
    "\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.set_ylabel(\"Lifetime\")\n",
    "    ax.set_title(\"Agent Lifetime Over Time\")\n",
    "    ax.legend()\n",
    "    plt.show()   \n",
    "\n",
    "def plot_lifetimes_from_save(agent_name: str, random_agent_lifetime: float | None = None, window: int = 50) -> None:\n",
    "    history = Jar(\"saves\").get(agent_name)\n",
    "\n",
    "    lifetimes = [episode[3] for episode in history]\n",
    "    num_agents = len(lifetimes[0])\n",
    "    \n",
    "    avg_lifetimes = [np.mean([r for r in reward.values()]) for reward in lifetimes]\n",
    "    max_lifetimes = [np.max([r for r in reward.values()]) for reward in lifetimes]\n",
    "    min_lifetimes = [np.min([r for r in reward.values()]) for reward in lifetimes]\n",
    "    \n",
    "    if num_agents > 30:\n",
    "        ninetieth_percentile = [np.percentile([r for r in lifetime.values()], 90) for lifetime in lifetimes]\n",
    "    else:\n",
    "        ninetieth_percentile = None\n",
    "    \n",
    "    plot_lifetimes(avg_lifetimes, max_lifetimes, min_lifetimes, ninetieth_percentile, random_agent_lifetime, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_observations_from_save(save_name: str, agent_ids: list[int]) -> list[Observations]:       \n",
    "    history = Jar(\"saves\").get(save_name)\n",
    "    observations = [ep_obs[agent_id] \n",
    "                   for ep in history \n",
    "                   for ep_obs, _ in ep[0] \n",
    "                   for agent_id in agent_ids\n",
    "                   if agent_id in ep_obs]\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config.Default()\n",
    "conf.set(\"PLAYER_N\", 32)\n",
    "conf.set(\"NPC_N\", 0)\n",
    "\n",
    "reward = ResourcesAndGatheringReward(1024)\n",
    "\n",
    "random_reward, random_rewards = get_avg_reward_for_random_agent(conf, reward=reward, retries=5)\n",
    "random_reward_std = np.std(random_rewards)\n",
    "\n",
    "random_lifetime, random_lifetimes = get_avg_lifetime_for_random_agent(conf, retries=5)\n",
    "random_lifetime_std = np.std(random_lifetimes)\n",
    "\n",
    "print(f\"Random agent reward: {random_reward:.6f} ± {random_reward_std:.6f}\")\n",
    "print(f\"Random agent lifetime: {random_lifetime:4.2f} ± {random_lifetime_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"copy_arrays_32_agents_npcs_quick_test\"\n",
    "save_name = agent_name\n",
    "\n",
    "train_ppo(nmmo.Env(conf),\n",
    "          SimplierInputAgent(\n",
    "              learning_rate=5e-5,\n",
    "              epsilon=0.1,\n",
    "              epochs=50,\n",
    "              batch_size=256),\n",
    "          episodes=20,\n",
    "          save_every=200,\n",
    "          print_every=1,\n",
    "          custom_reward=reward,\n",
    "          agent_name=agent_name,\n",
    "          callbacks=[SavingCallback(save_name, saved_agent_ids=list(range(32)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_from_save(save_name, random_agent_reward=random_reward, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lifetimes_from_save(save_name, random_agent_lifetime=random_lifetime, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_from_save(save_name, window=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = get_all_observations_from_save(save_name, agent_ids=list(range(32)))\n",
    "net_inputs = [observations_to_inputs_simplier(obs, device=\"cpu\") for obs in observations]\n",
    "\n",
    "tiles = [inp[0][0] for inp in net_inputs]\n",
    "tile_features = [feature for tile in tiles for feature in tile.reshape(-1, 28)[:, -9:]if not torch.all(feature == 0)]\n",
    "\n",
    "self_datas = [inp[1][0] for inp in net_inputs]\n",
    "move_masks = [inp[2][0] for inp in net_inputs]\n",
    "attack_masks = [inp[3][0] for inp in net_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_for_all(values, assertion_fn, description):\n",
    "    correct_count = sum([assertion_fn(tensor) for tensor in values])\n",
    "    total_count = len(values)\n",
    "    print(f\"{(description+':'):<35}{correct_count}/{total_count} {('✅' if correct_count == total_count else '❌')}\")\n",
    "\n",
    "assert_for_all(tiles, lambda x: x.shape == torch.Size([15, 15, 28]), \"Tiles shape\")\n",
    "assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, :16], dim=-1) == 1), \"16 features one-hot encoded\")\n",
    "assert_for_all(tiles, lambda x: torch.all(torch.sum(x[:, :, 16:18], dim=-1) == 1), \"Each tile either passable or not\")\n",
    "assert_for_all(tiles, lambda x: torch.all(torch.logical_or(x[:, :, 18] == 0, x[:, :, 18] == 1)), \"Each tile harvestable or not\")\n",
    "# TODO: Check seen entity data\n",
    "print()\n",
    "\n",
    "assert_for_all(self_datas, lambda x: x.shape == torch.Size([5]), \"Self data shape\")\n",
    "assert_for_all(self_datas, lambda x: torch.all((x >= 0) & (x <= 1)), \"All values between 0 and 1\")\n",
    "print()\n",
    "\n",
    "assert_for_all(attack_masks, lambda x: x.shape == torch.Size([3]), \"Attack mask shape\")\n",
    "assert_for_all(attack_masks, lambda x: torch.all(x == 1), \"Every attack style valid\")\n",
    "print()\n",
    "\n",
    "assert_for_all(move_masks, lambda x: x.shape == torch.Size([5]), \"Move mask shape\")\n",
    "assert_for_all(move_masks, lambda x: x[-1] == 1, \"Can not move\")\n",
    "assert_for_all(move_masks, lambda x: torch.any(x[:-1] == 1), \"Can move somewhere\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
